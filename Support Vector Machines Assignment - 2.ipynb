{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.** What is the relationship between polynomial functions and kernel functions in machine learning\n",
    "algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Polynomial Functions:**\n",
    "\n",
    "Polynomial functions are mathematical expressions that involve variables raised to various powers, often combined with coefficients.\n",
    "\n",
    "In machine learning, polynomial functions are employed to transform data into higher-dimensional spaces. For instance, in polynomial regression, these functions help capture nonlinear relationships within the data.\n",
    "\n",
    "**Kernel Functions:**\n",
    "\n",
    "Kernel functions are essential in kernel methods, such as SVMs. They measure the similarity between pairs of data points, potentially in higher-dimensional spaces.\n",
    "\n",
    "Various types of kernel functions exist, including linear, polynomial, Gaussian (RBF), and sigmoid kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Relationship:**\n",
    "\n",
    "Polynomial functions can serve as kernel functions in kernel methods.\n",
    "\n",
    "For example, the polynomial kernel is derived from the polynomial function. It calculates the similarity between two data points by raising the dot product of their features to a certain power and adding a constant.\n",
    "\n",
    "By using polynomial functions as kernels, we effectively map the data into higher-dimensional spaces, enabling linear methods to capture nonlinear patterns in the data.\n",
    "\n",
    "To sum up, polynomial functions can be utilized as kernel functions within kernel methods. This usage enables the nonlinear mapping of data into higher-dimensional spaces, facilitating the capture of complex relationships in machine learning tasks.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.** How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train SVM classifier with polynomial kernel\n",
    "svm_classifier = SVC(kernel='poly', degree=3)  # Polynomial kernel of degree 3\n",
    "svm_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.** How does increasing the value of epsilon affect the number of support vectors in SVR?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Support Vector Regression (SVR), the epsilon parameter (ϵ) defines the width of the epsilon-insensitive tube around the predicted values within which no penalty is associated with errors.\n",
    "\n",
    "Increasing the value of epsilon can affect the number of support vectors in SVR in the following way:\n",
    "\n",
    "**Fewer Support Vectors with Larger Epsilon:**\n",
    "\n",
    "As epsilon increases, the margin around the predicted values widens.\n",
    "\n",
    "This means that data points can be farther away from the regression line while still being considered correctly predicted, as long as they fall within the wider margin.\n",
    "\n",
    "Consequently, fewer data points are required to support the regression function within the wider margin.\n",
    "\n",
    "Thus, increasing epsilon typically leads to a decrease in the number of support vectors.\n",
    "\n",
    "**More Support Vectors with Smaller Epsilon:**\n",
    "\n",
    "Conversely, when epsilon is small, the margin around the predicted values narrows.\n",
    "\n",
    "This means that data points must be closer to the regression line to be considered correctly predicted.\n",
    "\n",
    "Consequently, more data points are needed to support the regression function within the narrower margin.\n",
    "\n",
    "Thus, decreasing epsilon typically leads to an increase in the number of support vectors.\n",
    "\n",
    "In summary, increasing the value of epsilon in SVR usually results in fewer support vectors, while decreasing epsilon leads to more support vectors. Adjusting epsilon allows for tuning the trade-off between model complexity (number of support vectors) and the width of the margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.** How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\n",
    "affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\n",
    "and provide examples of when you might want to increase or decrease its value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kernel Function:**\n",
    "\n",
    "The kernel function determines the type of mapping applied to the input space. Common choices include linear, polynomial, radial basis function (RBF), and sigmoid kernels.\n",
    "\n",
    "**Impact:**\n",
    "\n",
    "The choice of kernel function affects the model's ability to capture nonlinear relationships in the data.\n",
    "\n",
    "For example, if the data exhibits complex nonlinear patterns, using an RBF kernel might be more appropriate, while a linear kernel might suffice for simpler, linearly separable data.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "If the data has nonlinear patterns, such as in financial time series analysis or image recognition tasks, choosing a nonlinear kernel like RBF can improve model performance.\n",
    "\n",
    "**C Parameter:**\n",
    "\n",
    "The C parameter controls the trade-off between maximizing the margin and minimizing the training error. It represents the penalty for misclassification or deviation from the predicted values.\n",
    "\n",
    "**Impact:**\n",
    "\n",
    "A smaller C value encourages a wider margin and allows for more misclassifications or deviations, potentially leading to underfitting.\n",
    "\n",
    "A larger C value imposes a stricter penalty for misclassifications or deviations, potentially leading to overfitting.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "To prevent overfitting when the training data is noisy, you might decrease the C parameter to allow for a wider margin and tolerate more errors.\n",
    "\n",
    "**Epsilon Parameter:**\n",
    "\n",
    "The epsilon parameter (ϵ) defines the width of the epsilon-insensitive tube around the predicted values within which no penalty is associated with errors.\n",
    "\n",
    "**Impact:**\n",
    "\n",
    "A larger epsilon allows for a wider tube, meaning predictions within this range are considered accurate and do not contribute to the loss function.\n",
    "\n",
    "A smaller epsilon narrows the tube, making predictions need to be closer to the true values to be considered accurate.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "In cases where the output values have inherent noise or uncertainty, increasing epsilon can make the model more robust to such variations.\n",
    "\n",
    "**Gamma Parameter:**\n",
    "\n",
    "The gamma parameter determines the influence of a single training example, with low values meaning 'far' and high values meaning 'close'.\n",
    "\n",
    "**Impact:**\n",
    "\n",
    "A small gamma value implies a broader reach, where every training example has a far-reaching effect on the decision boundary.\n",
    "\n",
    "A large gamma value means that only nearby points have a significant influence on the decision boundary.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "When dealing with highly nonlinear data, a smaller gamma value might help avoid overfitting by considering a larger neighborhood of points.\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "Kernel Function: Choose based on the complexity of the data.\n",
    "\n",
    "C Parameter: Tune to balance margin width and training error penalty.\n",
    "\n",
    "Epsilon Parameter: Adjust to define the width of the insensitive tube.\n",
    "\n",
    "Gamma Parameter: Control the influence of individual training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5. Assignment:**\n",
    "\n",
    "Import the necessary libraries and load the dataseg\n",
    "\n",
    "Split the dataset into training and testing setZ\n",
    "\n",
    "Preprocess the data using any technique of your choice (e.g. scaling, normalization)\n",
    "\n",
    "Create an instance of the SVC classifier and train it on the training data\n",
    "\n",
    "Use the trained classifier to predict the labels of the testing data\n",
    "\n",
    "Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,precision, recall, F1-score)\n",
    "\n",
    "Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to improve its performance\n",
    "\n",
    "Train the tuned classifier on the entire dataset\n",
    "\n",
    "Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Best Parameters: {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tuned_svc_classifier.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data (scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "svc_classifier = SVC()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svc_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Use the trained classifier to predict the labels of the testing data\n",
    "y_pred = svc_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the performance of the classifier using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Tune hyperparameters using GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [0.1, 0.01, 0.001], 'kernel': ['linear', 'rbf', 'poly']}\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Print the best parameters found by GridSearchCV\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "tuned_classifier = SVC(**grid_search.best_params_)  # Using the best parameters found\n",
    "tuned_classifier.fit(X, y)\n",
    "\n",
    "# Save the trained classifier to a file\n",
    "joblib.dump(tuned_classifier, 'tuned_svc_classifier.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
